{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9649a3f9-957a-4045-ace7-f9a7930aa1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import logging\n",
    "import requests\n",
    "import openai\n",
    "from jira import JIRA\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Setup logging for better debugging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# OpenAI API key (set it securely, don't hard-code in production)\n",
    "openai.api_key = \"your-openai-api-key\"\n",
    "\n",
    "# JIRA Setup\n",
    "jira_url = \"https://your-domain.atlassian.net\"\n",
    "jira_user = \"your-email@example.com\"\n",
    "jira_token = \"your-jira-api-token\"\n",
    "jira = JIRA(jira_url, basic_auth=(jira_user, jira_token))\n",
    "\n",
    "# Setup Agentic AI API key\n",
    "AGENTIC_API_KEY = \"your-agentic-api-key\"  # Replace with your actual Agentic API key\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data_tool(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(f\"File {file_path} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype={\"Account\": \"category\", \"AU\": \"category\", \"Company\": \"category\"})\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading file {file_path}: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    required_cols = ['Asofdate', 'Company', 'Account', 'AU', 'Match Status', 'GL Balance', 'IHub balance', 'Balance difference']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        logging.error(f\"Missing columns {missing_cols}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    df.dropna(subset=required_cols, inplace=True)\n",
    "\n",
    "    # Encoding categorical variables\n",
    "    le_account = LabelEncoder()\n",
    "    le_au = LabelEncoder()\n",
    "    le_company = LabelEncoder()\n",
    "\n",
    "    df['Company'] = le_company.fit_transform(df['Company'].astype(str))\n",
    "    df['Account'] = le_account.fit_transform(df['Account'].astype(str))\n",
    "    df['AU'] = le_au.fit_transform(df['AU'].astype(str))\n",
    "    df['Match Status'] = LabelEncoder().fit_transform(df['Match Status'].astype(str))\n",
    "    df['Asofdate'] = pd.to_datetime(df['Asofdate'], errors='coerce')\n",
    "\n",
    "    return df, le_account, le_au, le_company\n",
    "\n",
    "# Feature Engineering with LLM enhancement\n",
    "def engineer_features_with_llm(df):\n",
    "    for col in ['GL Balance', 'IHub balance', 'Balance difference']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "    df = df.sort_values('Asofdate')\n",
    "    grouped = df.groupby(['Company', 'Account', 'AU'], group_keys=False)\n",
    "\n",
    "    df['Days_Since_Last'] = grouped['Asofdate'].diff().dt.days.fillna(0)\n",
    "    df['GL_Change'] = grouped['GL Balance'].diff().fillna(0)\n",
    "    df['IHub_Change'] = grouped['IHub balance'].diff().fillna(0)\n",
    "    df['Diff_Change'] = grouped['Balance difference'].diff().fillna(0)\n",
    "    df['GL_Std'] = grouped['GL Balance'].transform(lambda x: x.rolling(window=3, min_periods=1).std().fillna(0))\n",
    "    df['IHub_Std'] = grouped['IHub balance'].transform(lambda x: x.rolling(window=3, min_periods=1).std().fillna(0))\n",
    "\n",
    "    features = ['Company', 'Account', 'AU', 'Days_Since_Last', 'GL Balance', 'IHub balance', 'Balance difference',\n",
    "                'GL_Change', 'IHub_Change', 'Diff_Change', 'GL_Std', 'IHub_Std']\n",
    "    return df, features\n",
    "\n",
    "# Estimate contamination dynamically\n",
    "def estimate_contamination(df, feature='Balance difference', z_threshold=3):\n",
    "    mean_diff = df[feature].mean()\n",
    "    std_diff = df[feature].std()\n",
    "    extreme_values = (df[feature].abs() > (mean_diff + z_threshold * std_diff)).sum()\n",
    "    contamination = min(max(extreme_values / len(df), 0.01), 0.5)  # Ensure reasonable contamination level\n",
    "    logging.info(f\"Estimated contamination: {contamination:.4f}\")\n",
    "    return contamination\n",
    "\n",
    "# Train Isolation Forest model\n",
    "def train_model_tool(df, features):\n",
    "    X = df[features]\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    contamination = estimate_contamination(df)\n",
    "\n",
    "    model = IsolationForest(n_estimators=200, contamination=contamination, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_scaled)\n",
    "\n",
    "    df['Anomaly_Score'] = model.decision_function(X_scaled)\n",
    "    df['Anomaly'] = model.predict(X_scaled)\n",
    "    df['Anomaly'] = df['Anomaly'].apply(lambda x: 1 if x == -1 else 0)  # Convert -1 to 1 (anomaly), 1 to 0 (normal)\n",
    "\n",
    "    return model, df\n",
    "\n",
    "# Generate enhanced explanation using LLM\n",
    "def generate_enhanced_explanation(anomaly_data):\n",
    "    prompt = f\"\"\"\n",
    "    Anomaly Detected:\n",
    "    - GL Balance: {anomaly_data['GL Balance']}\n",
    "    - IHub Balance: {anomaly_data['IHub balance']}\n",
    "    - Balance Difference: {anomaly_data['Balance difference']}\n",
    "    - Days Since Last Transaction: {anomaly_data['Days_Since_Last']}\n",
    "    - GL Balance Change: {anomaly_data['GL_Change']}\n",
    "    - IHub Balance Change: {anomaly_data['IHub_Change']}\n",
    "    - Difference Change: {anomaly_data['Diff_Change']}\n",
    "    - GL Balance Standard Deviation: {anomaly_data['GL_Std']}\n",
    "    - IHub Balance Standard Deviation: {anomaly_data['IHub_Std']}\n",
    "   \n",
    "    Please provide an in-depth explanation for this anomaly. Include potential causes for discrepancies in the GL balance and IHub balance, taking into account changes over time, the relationship between the two, and any possible outliers. The anomaly might indicate accounting errors, data inconsistencies, or financial reporting issues.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate explanation using OpenAI's API\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    explanation = response['choices'][0]['text'].strip()\n",
    "    return explanation\n",
    "\n",
    "# Create task in Agentic AI\n",
    "def create_agentic_task(anomaly):\n",
    "    api_url = \"https://api.agentic.ai/tasks\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {AGENTIC_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Create task data based on the anomaly details\n",
    "    task_data = {\n",
    "        \"task_name\": f\"Investigate Anomaly: {anomaly['id']}\",\n",
    "        \"description\": anomaly['explanation'],\n",
    "        \"severity\": \"High\",  # Adjust severity based on anomaly characteristics\n",
    "        \"due_date\": \"2023-04-01\"  # Set a realistic due date\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(api_url, json=task_data, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            logging.info(f\"Task created successfully in Agentic AI.\")\n",
    "        else:\n",
    "            logging.error(f\"Error creating task in Agentic AI: {response.text}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception while creating task in Agentic AI: {str(e)}\")\n",
    "\n",
    "# Create JIRA ticket for anomaly\n",
    "def create_jira_ticket(anomaly):\n",
    "    summary = f\"Anomaly Detected: {anomaly['id']}\"\n",
    "    description = f\"Details of anomaly: {anomaly['description']}\\nSeverity: High\"\n",
    "\n",
    "    new_issue = jira.create_issue(\n",
    "        project='YOUR_PROJECT_KEY',\n",
    "        summary=summary,\n",
    "        description=description,\n",
    "        issuetype={'name': 'Task'}\n",
    "    )\n",
    "    logging.info(f\"JIRA ticket created: {new_issue.key}\")\n",
    "\n",
    "# Save anomalies to CSV\n",
    "def save_anomalies_to_csv(anomalies, file_name=\"anomalies_output.csv\"):\n",
    "    anomalies.to_csv(file_name, index=False)\n",
    "    logging.info(f\"Anomalies saved to {file_name}\")\n",
    "\n",
    "# Email anomaly results\n",
    "def send_email_tool(anomalies, recipient_email, le_account, le_au, le_company):\n",
    "    sender_email = \"youremailid\"\n",
    "    sender_password = \"your-app-password\"  # Replace with actual app password\n",
    "\n",
    "    if anomalies.empty:\n",
    "        logging.info(\"No anomalies detected. Skipping email.\")\n",
    "        return\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    anomalies = anomalies.copy()\n",
    "\n",
    "    # Fix dtype issues by explicitly converting to string before assignment\n",
    "    anomalies.loc[:, 'Company'] = le_company.inverse_transform(anomalies['Company'].astype(int)).astype(str)\n",
    "    anomalies.loc[:, 'Account'] = le_account.inverse_transform(anomalies['Account'].astype(int)).astype(str)\n",
    "    anomalies.loc[:, 'AU'] = le_au.inverse_transform(anomalies['AU'].astype(int)).astype(str)\n",
    "\n",
    "    # Generate enhanced explanations and tasks for anomalies\n",
    "    for idx, anomaly in anomalies.iterrows():\n",
    "        explanation = generate_enhanced_explanation(anomaly)\n",
    "        anomaly['explanation'] = explanation\n",
    "\n",
    "        # Create JIRA ticket and Agentic AI task\n",
    "        create_jira_ticket(anomaly)\n",
    "        create_agentic_task(anomaly)  # Create task in Agentic AI\n",
    "\n",
    "    # Save anomalies to CSV\n",
    "    save_anomalies_to_csv(anomalies)\n",
    "\n",
    "    subject = \"Anomalies Detected\"\n",
    "    body = anomalies.to_html()\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = recipient_email\n",
    "    msg['Subject'] = subject\n",
    "    msg.attach(MIMEText(body, 'html'))\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
    "            server.login(sender_email, sender_password)\n",
    "            server.sendmail(sender_email, recipient_email, msg.as_string())\n",
    "        logging.info(\"Email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to send email: {str(e)}\")\n",
    "\n",
    "# Feedback loop for anomaly detection reframing\n",
    "def feedback_loop_and_retrain(df, feedback_data, model, features):\n",
    "    \"\"\"\n",
    "    This function integrates feedback data to refine and retrain the anomaly detection model.\n",
    "    feedback_data: DataFrame containing feedback about the anomalies\n",
    "    \"\"\"\n",
    "    logging.info(\"Applying feedback to adjust anomaly detection model...\")\n",
    "\n",
    "    # Example of adjusting contamination based on feedback (e.g., if feedback indicates fewer anomalies)\n",
    "    positive_feedback = feedback_data[feedback_data['feedback'] == 1]  # Feedback = 1 means anomaly is real\n",
    "    negative_feedback = feedback_data[feedback_data['feedback'] == 0]  # Feedback = 0 means false positive\n",
    "\n",
    "    # Retrain model if necessary based on feedback (e.g., adjust contamination rate)\n",
    "    contamination = estimate_contamination(df)  # Can be adjusted based on feedback data\n",
    "\n",
    "    # Retrain model with new contamination value or other adjustments\n",
    "    model = IsolationForest(n_estimators=200, contamination=contamination, random_state=42, n_jobs=-1)\n",
    "    model.fit(df[features])\n",
    "    logging.info(\"Model retrained with feedback.\")\n",
    "    return model\n",
    "\n",
    "# Main Execution\n",
    "def main(file_path, feedback_file_path):\n",
    "    # Load the main data\n",
    "    df, le_account, le_au, le_company = load_data_tool(file_path)\n",
    "    \n",
    "    # Load feedback data from a separate file (e.g., feedback_file.csv)\n",
    "    feedback_data = pd.read_csv(feedback_file_path)  # Load feedback data from file\n",
    "\n",
    "    # Ensure that feedback data has the expected columns (if needed, you can add validation)\n",
    "    if 'feedback' not in feedback_data.columns:\n",
    "        logging.error(\"Feedback data must contain a 'feedback' column.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Feature engineering\n",
    "    df, features = engineer_features_with_llm(df)  # Updated function\n",
    "    \n",
    "    # Train initial model\n",
    "    model, df = train_model_tool(df, features)  # Train initial model\n",
    "\n",
    "    # Apply feedback loop to retrain the model\n",
    "    if feedback_data is not None and not feedback_data.empty:\n",
    "        model = feedback_loop_and_retrain(df, feedback_data, model, features)\n",
    "\n",
    "    # Send email with the detected anomalies\n",
    "    send_email_tool(df[df['Anomaly'] == 1], \"recipient@example.com\", le_account, le_au, le_company)\n",
    "\n",
    "\n",
    "# Example of providing file paths:\n",
    "file_path = \"/path/to/your/file.csv\"  # Main data file path\n",
    "feedback_file_path = \"/path/to/your/feedback_file.csv\"  # Feedback data file path\n",
    "\n",
    "# Call the main function with file paths\n",
    "main(file_path, feedback_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
